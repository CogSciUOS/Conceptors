{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptor Example II\n",
    "\n",
    "This is an advanced example compared to the other conceptor notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize']=(8.0,6.0)\n",
    "mpl.rcParams['font.size']=14\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we configure the network for a given size and connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network size\n",
    "N = 100\n",
    "\n",
    "# network connectivity\n",
    "netConn = 10./N\n",
    "#netConn = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Matrix\n",
    "The initial weight matrix is randomly generated and sparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# requires sprandn function which is not implemented in scipy\n",
    "import numpy.random\n",
    "import scipy.sparse\n",
    "\n",
    "def sprandn(m, n, density):\n",
    "    nnz = max(0, min(int(m*n*density), m*n))\n",
    "    seq = np.random.permutation(m*n)[:nnz]\n",
    "    data = np.random.randn(nnz)\n",
    "    return scipy.sparse.csr_matrix((data, (seq/n,seq%n)), shape=(m,n)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize random weights\n",
    "W_unscaled = sprandn(N,N,netConn)\n",
    "\n",
    "plt.matshow(W_unscaled, cmap=plt.cm.gray);\n",
    "plt.suptitle(\"Visualization of random sparse weight matrix\");\n",
    "plt.ylabel(\"Neuron\");\n",
    "plt.xlabel(\"Neuron\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scale the weight matrix such that the spectral radius (i.e. the largest eigenvalue) becomes 1. Therefore we first need to compute the spectral radius and divide by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "# compute the spectral radius (= largest eigenvalue)\n",
    "def getSpecRad(m):\n",
    "    specRad, largestEigenvec = np.abs(scipy.linalg.eigh(m,eigvals=(N-1, N-1)))\n",
    "    return specRad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "specRad = getSpecRad(W_unscaled)\n",
    "print \"Spectral radius of the random matrix: {}\".format(specRad)\n",
    "\n",
    "W_raw = W_unscaled/specRad * 1.5\n",
    "\n",
    "# the new spectral radius is 1.5\n",
    "print \"Now the spectral radius is {}\".format(getSpecRad(W_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Matrix\n",
    "Now we set the input weights to the reservoir. It is also randomly initialized with certain scaling factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netInputScaling = 1.5\n",
    "numInputDims = 1\n",
    "\n",
    "W_in = netInputScaling * np.random.normal(0,1,(N,numInputDims))\n",
    "\n",
    "plt.matshow(W_in.T, cmap=plt.cm.gray);\n",
    "plt.suptitle(\"Input connection weights\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bias Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biasScaling = 0.2\n",
    "\n",
    "W_bias = biasScaling * np.random.normal(0,1,(N,1))\n",
    "\n",
    "plt.matshow(np.asmatrix(W_bias.T), cmap=plt.cm.gray);\n",
    "plt.suptitle(\"Bias\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "We need these 3 matrices (weight, input, bias) for the computation of the update rule:\n",
    "\n",
    "$ x(n+1) = \\tanh (W x(n) + W_{in} p(n+1) + W_{bias} ) $\n",
    "\n",
    "Where W is the weight matrix, currently our W_raw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "Now we need input patterns to train the network on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sinewave_1 = lambda n: np.matrix([[np.sin(2 * np.pi * n / 8.8342522)]])\n",
    "\n",
    "x_vals = np.arange(0,40,0.2)\n",
    "plt.plot(x_vals,[sinewave_1(x)[0, 0] for x in x_vals], 'b');\n",
    "plt.suptitle(\"Sinewave input function with irrational wavelength\");\n",
    "plt.xlabel(\"t\");\n",
    "plt.ylabel(\"$\\sin (2\\pi t / 8.8342522)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting network states\n",
    "\n",
    "Now we let the network run and collect states from it. To get it \"swinging\" in the beginning, we add an additional \"washout\" period, during which we do not track the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "washoutTimesteps = 500\n",
    "learnTimesteps = 1000\n",
    "\n",
    "# init empty state\n",
    "x = np.zeros((N, 1))\n",
    "\n",
    "stateCollector = np.zeros((N, learnTimesteps))\n",
    "inputCollector = np.zeros((numInputDims, learnTimesteps))\n",
    "\n",
    "washoutStateCollector = np.zeros((N, washoutTimesteps))\n",
    "\n",
    "for t in xrange(washoutTimesteps + learnTimesteps):\n",
    "    # get input from pattern-function\n",
    "    u = sinewave_1(t)\n",
    "    \n",
    "    # update rule\n",
    "    x = np.tanh(np.dot(W_raw, x) + np.dot(W_in, u) + W_bias)\n",
    "    \n",
    "    # if we are over the washout period, we start saving states\n",
    "    if t >= washoutTimesteps:\n",
    "        # we need to transpose x and u,\n",
    "        # because accessing a row OR column in a numpy matrix always resturns a row vector\n",
    "        stateCollector[:, t - washoutTimesteps] = x.T\n",
    "        inputCollector[:, t - washoutTimesteps] = u.T\n",
    "    \n",
    "    # save washout state sequence as well for inspection\n",
    "    else:\n",
    "        washoutStateCollector[:, t] = x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the states over time\n",
    "plt.matshow(stateCollector[:,0:500], cmap=plt.cm.gray);\n",
    "plt.suptitle(\"Network state activation over time\");\n",
    "plt.xlabel(\"Timesteps\");\n",
    "plt.ylabel(\"Neuron\");\n",
    "\n",
    "# plot 5 randomly selected neurons\n",
    "plt.figure();\n",
    "neuron_idxs = np.random.permutation(range(N))[0:5]\n",
    "neuron_idxs.sort()\n",
    "for i, neuron in enumerate(neuron_idxs):\n",
    "    plt.plot(stateCollector[neuron ,0:40], label=\"Neuron #{}\".format(neuron));\n",
    "plt.suptitle(\"Activation of randomly selected neurons\");\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0.);\n",
    "plt.xlabel(\"Timestep\");\n",
    "plt.ylabel(\"Neuron activation\");\n",
    "\n",
    "    \n",
    "# plot input\n",
    "plt.figure();\n",
    "plt.plot(inputCollector[0,0:40]);\n",
    "plt.suptitle(\"Input signal\");\n",
    "plt.xlabel(\"Timestep\");\n",
    "plt.ylabel(\"Signal strength\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train output weights\n",
    "Train W_out such that it linearly combines the state activation to recreate the input sequence. For this we use the Tikhonov regularization (Ridge regression).\n",
    "\n",
    "### Tikhonov regularization\n",
    "The idea is to solve the equation $Ax=b$ for x under the constraint that the norm of $x$ stays small. This minimizes both the error $||Ax-b||^2$ and a regularization term $||\\alpha I||^2$ where $\\alpha$ is a factor of how much big weights are penalized.\n",
    "\n",
    "So $||Ax-b||^2 + ||\\alpha I||^2$ will be minimized.\n",
    "\n",
    "An explicit solution is $\\hat{x} = (A^T A + (\\alpha I)^T (\\alpha I))^{-1} A^T b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ridgeRegression(A, b, alpha):\n",
    "    aI = alpha * np.eye(A.shape[1])\n",
    "    first = np.linalg.inv(np.dot(A.T, A) + np.dot(aI.T, aI))\n",
    "    return np.dot(first , np.dot(A.T, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to plug in our values. Since for every state we want to generate the actual input as output of the network, for every timestep it holds:\n",
    "\n",
    "$state * W_{out} = input$\n",
    "\n",
    "So when we calculate $W_{out}$ we want it to work for every timestep, so we solve\n",
    "\n",
    "$stateSequence * W_{out} = inputSequence$\n",
    "\n",
    "for $W_{out}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we need to transpose out data to fit into the equation\n",
    "W_out = ridgeRegression(\n",
    "    A = stateCollector.T,\n",
    "    b = inputCollector.T,\n",
    "    alpha = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained output matrix\n",
    "Now we just append this phase after the washout+learning phase, such that the states that we test on are not the same we learned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTimesteps = 200\n",
    "\n",
    "# continue with last state\n",
    "x = x\n",
    "\n",
    "testOutputCollector = np.zeros((numInputDims, testTimesteps))\n",
    "testInputCollector = np.zeros((numInputDims, testTimesteps))\n",
    "\n",
    "\n",
    "for t in xrange(testTimesteps):\n",
    "    # get input from pattern-function after washout and learning phase\n",
    "    # this means we just continue after this phase\n",
    "    u = sinewave_1(washoutTimesteps + learnTimesteps + t)\n",
    "    \n",
    "    # update rule\n",
    "    x = np.tanh(np.dot(W_raw, x) + np.dot(W_in, u) + W_bias)\n",
    "    \n",
    "    # start computing outputs\n",
    "    output = np.dot(x.T, W_out)\n",
    "    testOutputCollector[:, t] = output.T\n",
    "    testInputCollector[:, t] = u.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot output from trained output matrix vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(testOutputCollector[0,0:20], 'r', label=\"Output\");\n",
    "plt.plot(testInputCollector[0,0:20], 'b', label=\"Input\");\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0.);\n",
    "plt.suptitle(\"Input output comparison\");\n",
    "plt.xlabel(\"Timestep\");\n",
    "plt.ylabel(\"Signal strength\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Error measure\n",
    "We use the NRMSE as defined by Wikipedia:\n",
    "\n",
    "$$NRMSE = \\frac{RMSE}{y_{max}-y_{min}}$$ where $$RMSE = \\sqrt{\\frac{1}{n}\\sum (\\hat{y}-y)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NRMSE(output, target):\n",
    "    error = target - output\n",
    "    \n",
    "    # ptp = range\n",
    "    peakToPeak = np.ptp(target, axis=1)\n",
    "    rmse = np.sqrt(np.mean(error**2, axis=1))\n",
    "    nrmse = rmse / peakToPeak\n",
    "    return nrmse\n",
    "\n",
    "print 'NRMSE Readout: {}'.format(NRMSE(testOutputCollector, testInputCollector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading\n",
    "Now we load the weight matrix. This means that the weight matrix is itself capable of producing the pattern in absence of an input to the network.\n",
    "\n",
    "Again we do a ridge regression. We want to find a new weight matrix W such that applying W to a state produces the activation that was previously produced from BOTH state- AND input-activation. So for every timestep it should hold:\n",
    "\n",
    "$$ x(n) \\cdot W_{new} = W_{old}\\cdot x(n) + W_{in}\\cdot u(n)$$\n",
    "\n",
    "With the regression we fit $W_{new}$ to otimize this for all timesteps. The target for the regression is therefore\n",
    "\n",
    "$$W_{old}\\cdot x(n) + W_{in}\\cdot u(n)$$\n",
    "\n",
    "To calculate it, we rearrange the update rule:\n",
    "\n",
    "$$ x(n+1) = \\tanh (W x(n) + W_{in} p(n+1) + W_{bias} ) $$\n",
    "\n",
    "$$W_{old}\\cdot x(n) + W_{in}\\cdot u(n) = \\tanh^{-1} (x(n+1)) - W_{bias}$$\n",
    "\n",
    "Now leaving away the input will still produce the same results just from letting the network run on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stateCollectorOld = np.zeros((N, learnTimesteps))\n",
    "stateCollectorOld[:,1:] = stateCollector[:,0:-1]\n",
    "\n",
    "target = np.arctanh(stateCollector) - np.tile(W_bias, (1, learnTimesteps))\n",
    "\n",
    "W_loaded = ridgeRegression(stateCollectorOld.T, target.T, 0.1).T\n",
    "\n",
    "# Herberts regression\n",
    "#xxT = np.dot(stateCollectorOld, stateCollectorOld.T) \n",
    "#W_loaded = np.dot(np.dot(np.linalg.inv(xxT +  0.01 * np.eye(N)), stateCollectorOld), target.T).T\n",
    "\n",
    "loadSpecRad = getSpecRad(W_loaded)\n",
    "print 'Spec Rad of the loaded weight matrix ' + str(loadSpecRad)\n",
    "\n",
    "# training error per neuron\n",
    "nrmse_loading = NRMSE(np.dot(W_loaded, stateCollectorOld), target)\n",
    "print 'mean NRMSE W: ' + str(np.mean(nrmse_loading))\n",
    "\n",
    "\n",
    "plt.matshow(W_loaded, cmap=plt.cm.gray);\n",
    "plt.suptitle(\"Loaded weight matrix\");\n",
    "plt.xlabel(\"Neuron\");\n",
    "plt.ylabel(\"Neuron\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptor\n",
    "We now build a filter for the state space that acts as a regularized identity mapping on the netowork activations under the current pattern. We will call this filter \"Conceptor\" of that pattern. The Conceptor optimises the compromise between not altering the network states (identity mapping) and at the same time having low weights (i.e being sparse: suppressing umimportant directions in the state space)\n",
    "\n",
    "In order to calculate the Conceptor Matrix we first compute the Covariance Matrix of the States the Neurons went through during the course of the learning phase. We then perform Singular Value Decomposition and then modify the Eigenvalues according to: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S_{new} = S \\cdot (S+\\alpha^{-2}I)^{-1})$\n",
    "\n",
    "Consequences:\n",
    "\n",
    "* Values in S_new are < 1\n",
    "* The bigger $\\alpha$, the smaller the values in S_new.\n",
    "* The bigger the previous value in S, the bigger the value in S_new will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I = np.eye(N)\n",
    "\n",
    "# Aperture alpha\n",
    "alpha = 10\n",
    "\n",
    "# State correlation matrix\n",
    "R = np.dot(stateCollector, stateCollector.T) / learnTimesteps\n",
    "plt.matshow(R, cmap=plt.cm.gray);\n",
    "plt.suptitle(\"State correlation matrix\");\n",
    "plt.xlabel(\"Neuron\");\n",
    "plt.ylabel(\"Neuron\");\n",
    "\n",
    "# What other matrices could we use? Covariance matrix? Cross-correlation matrix?\n",
    "\n",
    "U,S,V = np.linalg.svd(R, full_matrices=True) \n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1).set_title(\"original eigenvalues in log scale\")\n",
    "plt.plot(np.log(S), 'g')\n",
    "plt.subplot(1,2,2).set_title(\"original eigenvalues\")\n",
    "plt.plot(S, 'g')\n",
    "\n",
    "S = np.diag(S)    \n",
    "\n",
    "Snew = (np.dot(S,np.linalg.inv(S + (alpha**-2)*I)))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1).set_title(\"rescaled eigenvalues in log scale\")\n",
    "plt.plot(np.log(np.diagonal(Snew)), 'g')\n",
    "plt.subplot(1,2,2).set_title(\"rescaled eigenvalues\")\n",
    "plt.plot(np.diagonal(Snew), 'g')\n",
    "\n",
    "# svd backwards with new S\n",
    "C = np.dot(U,np.dot(Snew,V))\n",
    "\n",
    "plt.matshow(C, cmap=plt.cm.gray);\n",
    "plt.suptitle(\"Conceptor matrix\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the loading\n",
    "Now when we let the network run on its own, it should produce the signal again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadingTestTimesteps = 200\n",
    "\n",
    "# init random state\n",
    "x = 0.5*np.random.randn(N, 1)\n",
    "\n",
    "loadingTestOutputCollector = np.zeros((numInputDims, loadingTestTimesteps))\n",
    "loadingTestStateCollector = np.zeros((N, loadingTestTimesteps))\n",
    "\n",
    "for t in xrange(loadingTestTimesteps):\n",
    "    \n",
    "    # update rule\n",
    "    x = np.tanh(np.dot(W_loaded, x) + W_bias)\n",
    "    \n",
    "    # Apply the Concepor\n",
    "    x = np.dot(C,x)\n",
    "    \n",
    "    loadingTestStateCollector[:, t] = x.T\n",
    "    \n",
    "    output = np.dot(x.T, W_out)\n",
    "    loadingTestOutputCollector[:, t] = output.T\n",
    "\n",
    "\n",
    "\n",
    "plt.matshow(loadingTestStateCollector[:,:], cmap=plt.cm.gray);\n",
    "plt.suptitle(\"Autonomous running state activation\");\n",
    "plt.xlabel(\"Timestep\");\n",
    "plt.ylabel(\"Neuron\");\n",
    "\n",
    "plt.figure();\n",
    "plt.plot(loadingTestOutputCollector[0,:40], 'r', label=\"Output\");\n",
    "plt.plot(testInputCollector[0,:40], 'b', label=\"Original Signal\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0.);\n",
    "plt.suptitle(\"Comparison of autonomous running output with original signal\");\n",
    "plt.xlabel(\"Timestep\");\n",
    "plt.ylabel(\"Signal strength\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute NRMSE of test phase\n",
    "\n",
    "In order to compute this NRMSE we move the two signals along each other and get rid of the inevitable phase shift that influences the NRMSE. We also perform some interpolaion to smooth sharp edges which are due to our low sampling frequency. This code is not important to understand the basic conceptor and network algorihm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "# Plotting Input I and Output Y over plotTimesteps\n",
    "plotTimesteps = 100\n",
    "overSamplingFactor = 5\n",
    "\n",
    "I = inputCollector[:,0:plotTimesteps]\n",
    "Y = loadingTestOutputCollector  \n",
    "   \n",
    "fI = interpolate.interp1d(range(plotTimesteps),I,kind='cubic') \n",
    "fY = interpolate.interp1d(range(len(Y.T)),Y,kind='cubic') \n",
    "\n",
    "oversampledI = np.zeros((plotTimesteps-1)*overSamplingFactor)\n",
    "oversampledY = np.zeros((len(Y.T)-1)*overSamplingFactor)\n",
    "\n",
    "for i in range(len(oversampledI)):\n",
    "    oversampledI[i] = fI(i/overSamplingFactor) \n",
    "for i in range(len(oversampledY)):    \n",
    "    oversampledY[i] = fY(i/overSamplingFactor) \n",
    "L = len(oversampledY)\n",
    "M = len(oversampledI)\n",
    "\n",
    "#print L\n",
    "#print M\n",
    "\n",
    "phasematches = np.zeros(L-M)   \n",
    "for s in range(L-M):\n",
    "    phasematches[s] = np.linalg.norm(oversampledI-oversampledY[s:s+M])\n",
    "pos = np.argmin(phasematches)  \n",
    "print 'Position of best phase match: ' + str(pos)\n",
    "val = phasematches[pos]\n",
    "print 'Value of Frobenius Norm of differences between patterns with best phase match: ' + str(val)\n",
    "plt.plot(phasematches);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aligned Y for plotting\n",
    "Yidx = np.linspace(pos,pos+overSamplingFactor*(plotTimesteps-1)-1,plotTimesteps).astype(int)\n",
    "Y_PL = oversampledY[Yidx]\n",
    "\n",
    "#\n",
    "Iidx = np.linspace(0,overSamplingFactor*(plotTimesteps-1)-1,plotTimesteps).astype(int)\n",
    "I_PL = oversampledI[Iidx]\n",
    "\n",
    "NRMSEsAlign = NRMSE(np.reshape(Y_PL,(1,len(Y_PL))),np.reshape(I_PL,(1,len(I_PL))))        \n",
    "print 'NRMSE: ' + str(NRMSEsAlign)\n",
    "xspace = np.linspace(0,plotTimesteps,plotTimesteps)\n",
    "plt.plot(xspace,I_PL);\n",
    "plt.plot(xspace,Y_PL);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
